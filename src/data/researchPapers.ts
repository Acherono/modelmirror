
import { ResearchPaper } from "@/types/research";

export const researchPapers: ResearchPaper[] = [
  {
    id: "1",
    title: "Diffusion Learning for Visual Understanding",
    author: "John Doe",
    date: "2025-04-04",
    publisher: "arxiv.org",
    summary: "In this paper we argument the necessity of diffusion training for higher quality image generation. We explore new techniques for noise scheduling that improve sample quality and training stability.",
    category: "Vision",
    researchField: "Diffusion Models",
    sourceUrl: "https://arxiv.org"
  },
  {
    id: "2",
    title: "Transformer Scaling Laws in LLMs",
    author: "Jane Smith",
    date: "2025-03-15",
    publisher: "NeurIPS",
    summary: "We investigate the scaling properties of transformer models and identify new relationships between model size, data quantity, and performance on language tasks.",
    category: "NLP",
    researchField: "Large Language Models",
    sourceUrl: "https://neurips.cc"
  },
  {
    id: "3",
    title: "Multi-Modal Attention Mechanisms",
    author: "Alex Johnson",
    date: "2025-02-28",
    publisher: "ICML",
    summary: "This research presents novel attention mechanisms for processing multiple modalities simultaneously, allowing for better alignment between text, images, and audio.",
    category: "Multi-Modal",
    researchField: "Attention Mechanisms",
    sourceUrl: "https://icml.cc"
  },
  {
    id: "4",
    title: "Reinforcement Learning from Human Feedback",
    author: "Maria Garcia",
    date: "2025-02-10",
    publisher: "ACL",
    summary: "We demonstrate how reinforcement learning from human feedback can be effectively used to align language models with human values and intentions.",
    category: "Alignment",
    researchField: "RLHF",
    sourceUrl: "https://aclweb.org"
  },
  {
    id: "5",
    title: "Efficient Training of Billion-Parameter Models",
    author: "Raj Patel",
    date: "2025-01-22",
    publisher: "ICLR",
    summary: "This paper presents techniques for efficient training of extremely large neural networks, reducing compute requirements while maintaining model performance.",
    category: "Optimization",
    researchField: "Training Efficiency",
    sourceUrl: "https://iclr.cc"
  },
  {
    id: "6",
    title: "Zero-Shot Learning in Vision-Language Models",
    author: "Sarah Kim",
    date: "2025-01-05",
    publisher: "CVPR",
    summary: "Our research explores how vision-language models learn to perform new tasks without explicit training examples through careful pre-training strategies.",
    category: "Vision-Language",
    researchField: "Zero-Shot Learning",
    sourceUrl: "https://cvpr.thecvf.com"
  },
  {
    id: "7",
    title: "Self-Supervised Learning for Audio Recognition",
    author: "David Wilson",
    date: "2024-12-18",
    publisher: "InterSpeech",
    summary: "We present a novel self-supervised approach to training audio recognition models that achieves state-of-the-art performance on multiple benchmarks.",
    category: "Audio",
    researchField: "Self-Supervised Learning",
    sourceUrl: "https://interspeech.org"
  },
  {
    id: "8",
    title: "Emergent Abilities in Large Language Models",
    author: "Lisa Chen",
    date: "2024-12-03",
    publisher: "EMNLP",
    summary: "This study examines emergent abilities in large language models, identifying capabilities that appear only after reaching certain scale thresholds.",
    category: "NLP",
    researchField: "Emergence",
    sourceUrl: "https://emnlp.org"
  }
];
